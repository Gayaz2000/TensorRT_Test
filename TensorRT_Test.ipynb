{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM27CDk+r4gSip4foJCEAkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gayaz2000/TensorRT_Test/blob/main/TensorRT_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0U7fla3rXXc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NRIRyuaKXYOx",
        "outputId": "69b27496-7423-494b-a881-f286834309cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 21 09:43:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/machine-learning/tensorrt/secure/8.6.1/local_repos/nv-tensorrt-local-repo-ubuntu2204-8.6.1-cuda-11.8_1.0-1_amd64.deb -O tensorrt.deb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cCzFWcqdXlrx",
        "outputId": "bf45743e-bb8a-4567-faef-91f69b83cf6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-21 09:44:00--  https://developer.download.nvidia.com/compute/machine-learning/tensorrt/secure/8.6.1/local_repos/nv-tensorrt-local-repo-ubuntu2204-8.6.1-cuda-11.8_1.0-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.59.88.14, 23.59.88.2\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.59.88.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://developer.download.nvidia.com/403.html [following]\n",
            "--2025-07-21 09:44:00--  https://developer.download.nvidia.com/403.html\n",
            "Reusing existing connection to developer.download.nvidia.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252 [text/html]\n",
            "Saving to: ‘tensorrt.deb’\n",
            "\n",
            "\rtensorrt.deb          0%[                    ]       0  --.-KB/s               \rtensorrt.deb        100%[===================>]     252  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-21 09:44:00 (8.55 MB/s) - ‘tensorrt.deb’ saved [252/252]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo dpkg -i /content/tensorrt.deb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uggdTBxQXoGc",
        "outputId": "2aae66dd-bf63-4ecc-c787-75dc90abe84b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mdpkg-deb:\u001b[0m \u001b[1;31merror:\u001b[0m '/content/tensorrt.deb' is not a Debian format archive\n",
            "\u001b[1mdpkg:\u001b[0m error processing archive /content/tensorrt.deb (--install):\n",
            " dpkg-deb --control subprocess returned error exit status 2\n",
            "Errors were encountered while processing:\n",
            " /content/tensorrt.deb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo cp /var/nv-tensorrt-local-repo-*/nv-tensorrt-*-keyring.gpg /usr/share/keyrings/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V7myipxsX0a1",
        "outputId": "fa4f8ec7-bda7-45a7-c091-a7fbf77f7d21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/var/nv-tensorrt-local-repo-*/nv-tensorrt-*-keyring.gpg': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "wmg1OGIyX2wY",
        "outputId": "9a96f47e-722b-41d5-ea07-9ccd9f5657a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r            \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,840 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,762 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,932 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,126 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,267 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,148 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,461 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,139 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,572 kB]\n",
            "Fetched 33.6 MB in 3s (11.7 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libnvinfer-bin libnvinfer-dev libnvinfer-dispatch-dev libnvinfer-dispatch10\n",
            "  libnvinfer-headers-dev libnvinfer-headers-plugin-dev\n",
            "  libnvinfer-headers-python-plugin-dev libnvinfer-lean-dev libnvinfer-lean10\n",
            "  libnvinfer-plugin-dev libnvinfer-plugin10 libnvinfer-samples\n",
            "  libnvinfer-vc-plugin-dev libnvinfer-vc-plugin10\n",
            "  libnvinfer-win-builder-resource10 libnvinfer10 libnvonnxparsers-dev\n",
            "  libnvonnxparsers10 python3-libnvinfer python3-libnvinfer-dev\n",
            "  python3-libnvinfer-dispatch python3-libnvinfer-lean\n",
            "The following NEW packages will be installed:\n",
            "  libnvinfer-bin libnvinfer-dev libnvinfer-dispatch-dev libnvinfer-dispatch10\n",
            "  libnvinfer-headers-dev libnvinfer-headers-plugin-dev\n",
            "  libnvinfer-headers-python-plugin-dev libnvinfer-lean-dev libnvinfer-lean10\n",
            "  libnvinfer-plugin-dev libnvinfer-plugin10 libnvinfer-samples\n",
            "  libnvinfer-vc-plugin-dev libnvinfer-vc-plugin10\n",
            "  libnvinfer-win-builder-resource10 libnvinfer10 libnvonnxparsers-dev\n",
            "  libnvonnxparsers10 python3-libnvinfer python3-libnvinfer-dev\n",
            "  python3-libnvinfer-dispatch python3-libnvinfer-lean tensorrt\n",
            "0 upgraded, 23 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 4,521 MB of archives.\n",
            "After this operation, 11.6 GB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer10 10.12.0.36-1+cuda12.9 [1,229 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-lean10 10.12.0.36-1+cuda12.9 [16.6 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-plugin10 10.12.0.36-1+cuda12.9 [15.0 MB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-vc-plugin10 10.12.0.36-1+cuda12.9 [243 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dispatch10 10.12.0.36-1+cuda12.9 [222 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvonnxparsers10 10.12.0.36-1+cuda12.9 [1,365 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-bin 10.12.0.36-1+cuda12.9 [487 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-dev 10.12.0.36-1+cuda12.9 [111 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dev 10.12.0.36-1+cuda12.9 [2,023 MB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dispatch-dev 10.12.0.36-1+cuda12.9 [126 kB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-plugin-dev 10.12.0.36-1+cuda12.9 [6,052 B]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-python-plugin-dev 10.12.0.36-1+cuda12.9 [6,760 B]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-lean-dev 10.12.0.36-1+cuda12.9 [263 MB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-plugin-dev 10.12.0.36-1+cuda12.9 [16.8 MB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-vc-plugin-dev 10.12.0.36-1+cuda12.9 [104 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvonnxparsers-dev 10.12.0.36-1+cuda12.9 [2,095 kB]\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-samples 10.12.0.36-1+cuda12.9 [187 MB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-win-builder-resource10 10.12.0.36-1+cuda12.9 [763 MB]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer 10.12.0.36-1+cuda12.9 [793 kB]\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer-lean 10.12.0.36-1+cuda12.9 [512 kB]\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer-dispatch 10.12.0.36-1+cuda12.9 [512 kB]\n",
            "Get:22 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer-dev 10.12.0.36-1+cuda12.9 [2,962 B]\n",
            "Get:23 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  tensorrt 10.12.0.36-1+cuda12.9 [2,974 B]\n",
            "Fetched 4,521 MB in 59s (76.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 23.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnvinfer10.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libnvinfer10_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer10 (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-lean10.\n",
            "Preparing to unpack .../01-libnvinfer-lean10_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-lean10 (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-plugin10.\n",
            "Preparing to unpack .../02-libnvinfer-plugin10_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin10 (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-vc-plugin10.\n",
            "Preparing to unpack .../03-libnvinfer-vc-plugin10_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-vc-plugin10 (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-dispatch10.\n",
            "Preparing to unpack .../04-libnvinfer-dispatch10_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-dispatch10 (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvonnxparsers10.\n",
            "Preparing to unpack .../05-libnvonnxparsers10_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvonnxparsers10 (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-bin.\n",
            "Preparing to unpack .../06-libnvinfer-bin_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-bin (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-headers-dev.\n",
            "Preparing to unpack .../07-libnvinfer-headers-dev_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-headers-dev (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-dev.\n",
            "Preparing to unpack .../08-libnvinfer-dev_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-dev (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-dispatch-dev.\n",
            "Preparing to unpack .../09-libnvinfer-dispatch-dev_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-dispatch-dev (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-headers-plugin-dev.\n",
            "Preparing to unpack .../10-libnvinfer-headers-plugin-dev_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-headers-plugin-dev (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-headers-python-plugin-dev.\n",
            "Preparing to unpack .../11-libnvinfer-headers-python-plugin-dev_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-headers-python-plugin-dev (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-lean-dev.\n",
            "Preparing to unpack .../12-libnvinfer-lean-dev_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-lean-dev (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-plugin-dev.\n",
            "Preparing to unpack .../13-libnvinfer-plugin-dev_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin-dev (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-vc-plugin-dev.\n",
            "Preparing to unpack .../14-libnvinfer-vc-plugin-dev_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvinfer-vc-plugin-dev (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvonnxparsers-dev.\n",
            "Preparing to unpack .../15-libnvonnxparsers-dev_10.12.0.36-1+cuda12.9_amd64.deb ...\n",
            "Unpacking libnvonnxparsers-dev (10.12.0.36-1+cuda12.9) ...\n",
            "Selecting previously unselected package libnvinfer-samples.\n",
            "Preparing to unpack .../16-libnvinfer-samples_10.12.0.36-1+cuda12.9_all.deb ...\n",
            "Unpacking libnvinfer-samples (10.12.0.36-1+cuda12.9) ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-4109823874.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sudo apt-get update'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sudo apt-get install tensorrt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mBq4x2h-XZM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvidia-pyindex\n",
        "!pip install nvidia-tensorrt --extra-index-url https://pypi.nvidia.com"
      ],
      "metadata": {
        "id": "DA32PnsIXkaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "print(trt.__version__)"
      ],
      "metadata": {
        "id": "jBL3pXB1XZsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hls5g-kQtLo1",
        "outputId": "0a77b98e-7340-40c2-a89a-1807304a510c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.7.1\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (4.14.1)\n",
            "Collecting sympy>=1.13.3 (from torch==2.7.1)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch==2.7.1)\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch==2.7.1) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.1) (3.0.2)\n",
            "Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m129.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 torchaudio-2.7.1 torchvision-0.22.1 triton-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.7.1 torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get -y install libopenmpi-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UgBcPdytWRS",
        "outputId": "e805c3a3-303d-4fe1-bc13-d93475bcada6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/TensorRT-LLM.git\n",
        "!cd TensorRT-LLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHOxP8omCjla",
        "outputId": "29b50269-bd5a-437d-f988-9667a6b3b3f4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TensorRT-LLM'...\n",
            "remote: Enumerating objects: 97401, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 97401 (delta 23), reused 11 (delta 11), pack-reused 97342 (from 2)\u001b[K\n",
            "Receiving objects: 100% (97401/97401), 1.60 GiB | 30.72 MiB/s, done.\n",
            "Resolving deltas: 100% (71433/71433), done.\n",
            "Updating files: 100% (5908/5908), done.\n",
            "Filtering content: 100% (2377/2377), 1.79 GiB | 26.48 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all files in the mounted path\n",
        "os.listdir('/content/TensorRT-LLM')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uT61WLPTzyb",
        "outputId": "a365c0ff-6868-4038-9e10-06328d8a1474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.pre-commit-config.yaml',\n",
              " '.git',\n",
              " 'tensorrt_llm',\n",
              " 'LICENSE',\n",
              " 'setup.py',\n",
              " 'scripts',\n",
              " 'requirements-dev.txt',\n",
              " 'cpp',\n",
              " 'CONTRIBUTING.md',\n",
              " 'triton_backend',\n",
              " 'CODING_GUIDELINES.md',\n",
              " '.dockerignore',\n",
              " '.clang-tidy',\n",
              " 'tests',\n",
              " '.devcontainer',\n",
              " 'README.md',\n",
              " 'CODE_OF_CONDUCT.md',\n",
              " '.cursorignore',\n",
              " 'docs',\n",
              " 'requirements.txt',\n",
              " '.clangd',\n",
              " '.github',\n",
              " 'jenkins',\n",
              " 'constraints.txt',\n",
              " '.coderabbit.yaml',\n",
              " 'benchmarks',\n",
              " 'pyproject.toml',\n",
              " '.gitignore',\n",
              " '.gitmodules',\n",
              " '3rdparty',\n",
              " 'examples',\n",
              " '.clang-format',\n",
              " 'docker',\n",
              " '.gitattributes']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TensorRT-LLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI_SxYycX2ZV",
        "outputId": "b5e5cba8-72a7-405b-8e21-a2519d187957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TensorRT-LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mEEkc8aAV5rG",
        "outputId": "dbc932ea-7b8a-4f69-ceb9-294a772d72bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu128\n",
            "Requirement already satisfied: accelerate>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.2.2.post1)\n",
            "Requirement already satisfied: colored in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.3.0)\n",
            "Requirement already satisfied: cuda-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (12.6.2.post1)\n",
            "Requirement already satisfied: diffusers>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.34.0)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.2.2)\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (4.1.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.18.0)\n",
            "Requirement already satisfied: onnx_graphsurgeon>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.5.8)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.96.1)\n",
            "Requirement already satisfied: polygraphy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.49.26)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (5.9.5)\n",
            "Requirement already satisfied: nvidia-ml-py<13,>=12 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (12.575.51)\n",
            "Requirement already satisfied: pynvml==12.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (12.0.0)\n",
            "Requirement already satisfied: pulp in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (3.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (2.2.2)\n",
            "Requirement already satisfied: h5py==3.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (3.12.1)\n",
            "Requirement already satisfied: StrEnum in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (0.4.15)\n",
            "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (0.2.0)\n",
            "Requirement already satisfied: tensorrt~=10.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (10.11.0.33)\n",
            "Requirement already satisfied: torch<=2.8.0a0,>=2.7.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (2.7.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (0.22.1)\n",
            "Requirement already satisfied: nvidia-modelopt~=0.33.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt[torch]~=0.33.0->-r requirements.txt (line 28)) (0.33.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (2.26.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (12.6.77)\n",
            "Requirement already satisfied: transformers==4.53.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (4.53.1)\n",
            "Requirement already satisfied: pydantic>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (2.11.7)\n",
            "Requirement already satisfied: pydantic-settings in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (2.10.1)\n",
            "Requirement already satisfied: pillow==10.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (10.3.0)\n",
            "Requirement already satisfied: wheel<=0.45.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (0.45.1)\n",
            "Requirement already satisfied: optimum in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (1.26.1)\n",
            "Requirement already satisfied: datasets==3.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (3.1.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (0.4.5)\n",
            "Requirement already satisfied: mpmath>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (8.2.1)\n",
            "Requirement already satisfied: click_option_group in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (0.5.7)\n",
            "Requirement already satisfied: aenum in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (3.1.16)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (24.0.1)\n",
            "Requirement already satisfied: fastapi==0.115.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (0.115.4)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 46)) (0.35.0)\n",
            "Requirement already satisfied: setuptools<80 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 47)) (79.0.1)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 48)) (4.1.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 49)) (0.16.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (0.8.1)\n",
            "Requirement already satisfied: flashinfer-python==0.2.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 51)) (0.2.5)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 52)) (4.11.0.86)\n",
            "Requirement already satisfied: xgrammar==0.1.19 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 53)) (0.1.19)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (2.2.1)\n",
            "Requirement already satisfied: nvtx in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 55)) (0.2.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 56)) (3.10.0)\n",
            "Requirement already satisfied: meson in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 57)) (1.8.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 58)) (1.11.1.4)\n",
            "Requirement already satisfied: etcd3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 59)) (0.12.0)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 60)) (1.0.5)\n",
            "Requirement already satisfied: llguidance==0.7.29 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 61)) (0.7.29)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 62)) (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.1->-r requirements.txt (line 31)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.1->-r requirements.txt (line 31)) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.1->-r requirements.txt (line 31)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.1->-r requirements.txt (line 31)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.1->-r requirements.txt (line 31)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.1->-r requirements.txt (line 31)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.1->-r requirements.txt (line 31)) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.1->-r requirements.txt (line 31)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.1->-r requirements.txt (line 31)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0->-r requirements.txt (line 38)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0->-r requirements.txt (line 38)) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0->-r requirements.txt (line 38)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0->-r requirements.txt (line 38)) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0->-r requirements.txt (line 38)) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0->-r requirements.txt (line 38)) (3.11.15)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.4->-r requirements.txt (line 45)) (0.41.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.4->-r requirements.txt (line 45)) (4.14.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from xgrammar==0.1.19->-r requirements.txt (line 53)) (0.9.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from xgrammar==0.1.19->-r requirements.txt (line 53)) (3.3.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.27.0->-r requirements.txt (line 7)) (8.7.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.12.0->-r requirements.txt (line 11)) (5.29.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 13)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 13)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 13)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 13)) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 20)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 20)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 20)) (2025.2)\n",
            "Requirement already satisfied: tensorrt-cu12==10.11.0.33 in /usr/local/lib/python3.11/dist-packages (from tensorrt~=10.11.0->-r requirements.txt (line 24)) (10.11.0.33)\n",
            "Requirement already satisfied: tensorrt-cu12-libs==10.11.0.33 in /usr/local/lib/python3.11/dist-packages (from tensorrt-cu12==10.11.0.33->tensorrt~=10.11.0->-r requirements.txt (line 24)) (10.11.0.33)\n",
            "Requirement already satisfied: tensorrt-cu12-bindings==10.11.0.33 in /usr/local/lib/python3.11/dist-packages (from tensorrt-cu12==10.11.0.33->tensorrt~=10.11.0->-r requirements.txt (line 24)) (10.11.0.33)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from tensorrt-cu12-libs==10.11.0.33->tensorrt-cu12==10.11.0.33->tensorrt~=10.11.0->-r requirements.txt (line 24)) (12.6.77)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (1.11.1.6)\n",
            "Requirement already satisfied: nvidia-modelopt-core==0.33.0 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->-r requirements.txt (line 28)) (0.33.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->-r requirements.txt (line 28)) (13.9.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->-r requirements.txt (line 28)) (1.15.3)\n",
            "Requirement already satisfied: torchprofile>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->-r requirements.txt (line 28)) (0.0.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.1->-r requirements.txt (line 32)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.1->-r requirements.txt (line 32)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.1->-r requirements.txt (line 32)) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings->-r requirements.txt (line 33)) (1.1.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->-r requirements.txt (line 46)) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 56)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 56)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 56)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 56)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 56)) (3.2.3)\n",
            "Requirement already satisfied: grpcio>=1.27.1 in /usr/local/lib/python3.11/dist-packages (from etcd3->-r requirements.txt (line 59)) (1.73.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from etcd3->-r requirements.txt (line 59)) (1.17.0)\n",
            "Requirement already satisfied: tenacity>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from etcd3->-r requirements.txt (line 59)) (8.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->-r requirements.txt (line 62)) (1.17.1)\n",
            "\u001b[33mWARNING: nvidia-modelopt 0.33.0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 13)) (3.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 62)) (2.22)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0->-r requirements.txt (line 38)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0->-r requirements.txt (line 38)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0->-r requirements.txt (line 38)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0->-r requirements.txt (line 38)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0->-r requirements.txt (line 38)) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0->-r requirements.txt (line 38)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0->-r requirements.txt (line 38)) (1.20.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 13)) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 13)) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.1->-r requirements.txt (line 31)) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.1->-r requirements.txt (line 31)) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.1->-r requirements.txt (line 31)) (2.4.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers>=0.27.0->-r requirements.txt (line 7)) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.8.0a0,>=2.7.1->-r requirements.txt (line 26)) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->-r requirements.txt (line 28)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->-r requirements.txt (line 28)) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->-r requirements.txt (line 28)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cmake ninja-build build-essential"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAGkpfZuYX_T",
        "outputId": "05ba1779-c6fb-4c16-f6da-0f1752d2f586",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,840 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,764 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,461 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,124 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,932 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,139 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,572 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,267 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,148 kB]\n",
            "Fetched 33.6 MB in 3s (11.0 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "The following NEW packages will be installed:\n",
            "  ninja-build\n",
            "0 upgraded, 1 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 111 kB of archives.\n",
            "After this operation, 358 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ninja-build amd64 1.10.1-1 [111 kB]\n",
            "Fetched 111 kB in 0s (239 kB/s)\n",
            "Selecting previously unselected package ninja-build.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../ninja-build_1.10.1-1_amd64.deb ...\n",
            "Unpacking ninja-build (1.10.1-1) ...\n",
            "Setting up ninja-build (1.10.1-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TensorRT-LLM\n",
        "!pip install . --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIeEYGOAW9bf",
        "outputId": "f1cfc50b-0c45-415b-aab4-999a1f393487",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TensorRT-LLM\n",
            "Processing /content/TensorRT-LLM\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/TensorRT-LLM/tensorrt_llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUmzg2nSC9tn",
        "outputId": "22b44e5d-b131-4a35-bb4c-e0cc9bfd75cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Directory '/content/TensorRT-LLM/tensorrt_llm' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization"
      ],
      "metadata": {
        "id": "m0iFrOL9Q6nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorrt_llm.llmapi import QuantConfig, QuantAlgo\n",
        "\n",
        "quant_config = QuantConfig(quant_algo=QuantAlgo.W4A16_AWQ)\n",
        "\n",
        "llm = LLM(<model-dir>, quant_config=quant_config)"
      ],
      "metadata": {
        "id": "H-RwhKlDQ8Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "wplJt8_6RIzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorrt_llm.llmapi import LLM, SamplingParams, BuildConfig\n",
        "\n",
        "build_config = BuildConfig()\n",
        "build_config.max_beam_width = 4\n",
        "\n",
        "llm = LLM(<llama_model_path>, build_config=build_config)\n",
        "# Let the LLM object generate text with the default sampling strategy, or\n",
        "# you can create a SamplingParams object as well with several fields set manually\n",
        "sampling_params = SamplingParams(beam_width=4) # current limitation: beam_width should be equal to max_beam_width\n",
        "\n",
        "for output in llm.generate(<prompt>, sampling_params=sampling_params):\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "nF0a6c_ARKXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Configuration"
      ],
      "metadata": {
        "id": "AgV4GTj2RZoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(<model-path>,\n",
        "          build_config=BuildConfig(\n",
        "            max_num_tokens=4096,\n",
        "            max_batch_size=128,\n",
        "            max_beam_width=4))"
      ],
      "metadata": {
        "id": "xaljYSVhRaoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runtime Customization"
      ],
      "metadata": {
        "id": "sSpF1wLBTAXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorrt_llm.llmapi import LLM, KvCacheConfig\n",
        "\n",
        "llm = LLM(<llama_model_path>,\n",
        "          kv_cache_config=KvCacheConfig(\n",
        "            free_gpu_memory_fraction=0.8))"
      ],
      "metadata": {
        "id": "Fys-_jirTCeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer Customization"
      ],
      "metadata": {
        "id": "Mf6pCdRvUFji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(<llama_model_path>, tokenizer=<my_faster_one>)\n",
        "\n",
        "llm = LLM(<llama_model_path>)\n",
        "\n",
        "# for output in llm.generate([32, 12]):\n",
        "#     ..."
      ],
      "metadata": {
        "id": "T3-15uEtUK08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disable Tokenize"
      ],
      "metadata": {
        "id": "r5BpeaYfUbuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(<llama_model_path>)\n",
        "for output in llm.generate([[32, 12]], skip_tokenizer_init=True):\n",
        "    print(output)\n",
        "\n",
        "RequestOutput(\n",
        "    request_id=1,\n",
        "    prompt=None,\n",
        "    prompt_token_ids=[1, 15043, 29892, 590, 1024, 338],\n",
        "    outputs=[\n",
        "        CompletionOutput(\n",
        "            index=0, text='',\n",
        "            token_ids=[518, 10858, 4408, 29962, 322, 306, 626, 263, 518, 10858, 20627, 29962, 472, 518, 10858, 6938, 1822, 306, 626, 5007, 304, 4653, 590, 4066, 297, 278, 518, 11947, 18527, 29962, 2602, 472],\n",
        "            cumulative_logprob=None,\n",
        "            logprobs=[]\n",
        "            )\n",
        "        ], finished=True\n",
        "    )"
      ],
      "metadata": {
        "id": "QXtT6SXPUclO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "eJotdNNJU0nE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asyncio-Based Generation"
      ],
      "metadata": {
        "id": "RcJQOLxGU7vA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model=<llama_model_path>)\n",
        "\n",
        "async for output in llm.generate_async(<prompt>, streaming=True):\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "0FxnT7wyU3xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future-Style Generation"
      ],
      "metadata": {
        "id": "hViUSrG2U_yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will not block the main thread\n",
        "generation = llm.generate_async(<prompt>)\n",
        "# Do something else here\n",
        "# call .result() to explicitly block the main thread and wait for the result when needed\n",
        "output = generation.result()\n",
        "\n",
        "output = generation.result(timeout=10)\n",
        "\n",
        "generation = llm.generate_async(<prompt>)\n",
        "output = await generation.aresult()"
      ],
      "metadata": {
        "id": "szwgJLxRVCLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text"
      ],
      "metadata": {
        "id": "yfOd2NILV4KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorrt_llm import SamplingParams\n",
        "from tensorrt_llm._tensorrt_engine import LLM\n",
        "\n",
        "def main():\n",
        "  prompts = [\n",
        "        \"Hello, my name is\",\n",
        "        \"The capital of France is\",\n",
        "        \"The future of AI is\",\n",
        "    ]\n",
        "  sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "  llm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "  outputs = llm.generate(prompts, sampling_params)\n",
        "  for output in outputs:\n",
        "        prompt = output.prompt\n",
        "        generated_text = output.outputs[0].text\n",
        "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "JVwYkHBUFLqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text in streaming"
      ],
      "metadata": {
        "id": "grlbdVtHZprr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorrt_llm import LLM, SamplingParams\n",
        "import asyncio\n",
        "\n",
        "def main():\n",
        "  \"\"\"Geneating text in streaming\"\"\"\n",
        "  llm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "  prompts = [\n",
        "        \"Hello, my name is\",\n",
        "        \"The capital of France is\",\n",
        "        \"The future of AI is\",\n",
        "    ]\n",
        "  sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "  async def task(id, prompt):\n",
        "    async for output in llm.generate_async(prompt, sampling_params=sampling_params, streaming=True):\n",
        "        print(f\"Generation for prompt-{id}: {output.outputs[0].text!r}\")\n",
        "  async def main():\n",
        "    tasks = [task(id, prompt) for id, prompt in enumerate(prompts)]\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "  asyncio.run(main())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "NPlZIxEnZxHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed LLM Generation"
      ],
      "metadata": {
        "id": "y_bHjJkEbbC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from tensorrt_llm import LLM, SamplingParams\n",
        "\n",
        "async def main():\n",
        "  \"\"\"Distributed Generation\"\"\"\n",
        "  llm = LLM(\n",
        "      model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "      tensor_parallelism_size = 2, # Enable 2-way Tensor Parallelism\n",
        "      #pipeline_parallel_size=2, #Enable 2-way Pipeline Parallelism\n",
        "      #moe_expert_parallel_size = 2, # Enable 2-way MoE Expert Parallelism\n",
        "      #moe_tensor_parallel_size = 2, # Enable 2-way MoE Tensor Parallelism\n",
        "      )\n",
        "  prompts = [\n",
        "        \"Hello, my name is\",\n",
        "        \"The capital of France is\",\n",
        "        \"The future of AI is\",\n",
        "    ]\n",
        "  sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "  async for output in llm.generate_async(prompts, sampling_params=sampling_params):\n",
        "    print(f\"Prompt: {output.prompt!r}, Generated text: {output.outputs[0].text!r}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "aiPYKM9zbbq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text with guided decoding"
      ],
      "metadata": {
        "id": "QL24FpjfdQyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from tensorrt_llm import LLM, SamplingParams\n",
        "from tensorrt_llm.llmapi import GuidedDecodingParams\n",
        "\n",
        "async def main():\n",
        "  \"\"\"Generate text with guided decoding\"\"\"\n",
        "  llm = LLM(\n",
        "      model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "      guided_decoding_backend = \"xgrammar\",\n",
        "      disable_overlap_scheduler=True\n",
        "      )\n",
        "  schema = '{\n",
        "    \"title\": \"WirelessAccessPoint\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"ssid\": {\"title\": \"SSID\", \"type\": \"string\"},\n",
        "        \"securityProtocol\": {\"title\": \"SecurityProtocol\", \"type\": \"string\"},\n",
        "        \"bandwidth\": {\"title\": \"Bandwidth\", \"type\": \"string\"}\n",
        "        },\n",
        "    \"required\": [\"ssid\", \"securityProtocol\", \"bandwidth\"]\n",
        "    }'\n",
        "  prompt = [\n",
        "      {\n",
        "      'role': 'system',\n",
        "      'content': \"You are a helpful assistant that answers in JSON. Here's the json schema you must adhere to:\\n<schema>\\n{'title': 'WirelessAccessPoint', 'type': 'object', 'properties': {'ssid': {'title': 'SSID', 'type': 'string'}, 'securityProtocol': {'title': 'SecurityProtocol', 'type': 'string'}, 'bandwidth': {'title': 'Bandwidth', 'type': 'string'}}, 'required': ['ssid', 'securityProtocol', 'bandwidth']}\\n</schema>\\n\"\n",
        "      },\n",
        "      {\n",
        "      'role': 'user',\n",
        "      'content':\"I'm currently configuring a wireless access point for our office network and I need to generate a JSON object that accurately represents its settings. The access point's SSID should be 'OfficeNetSecure', it uses WPA2-Enterprise as its security protocol, and it's capable of a bandwidth of up to 1300 Mbps on the 5 GHz band. This JSON object will be used to document our network configurations and to automate the setup process for additional access points in the future. Please provide a JSON object that includes these details.\"\n",
        "      }\n",
        "  ]\n",
        "  prompt = llm.tokenizer.apply_chat_template(prompt, tokenize=False)\n",
        "  print(f\"Prompt: {prompt!r}\")\n",
        "\n",
        "  output = llm.generate(prompt, sampling_params=SamplingParams(max_tokens=50))\n",
        "  print(f\"Generated text (unguided): {output.outputs[0].text!r}\")\n",
        "\n",
        "  output = llm.generate(\n",
        "        prompt,\n",
        "        sampling_params=SamplingParams(\n",
        "            max_tokens=50, guided_decoding=GuidedDecodingParams(json=schema)))\n",
        "  print(f\"Generated text (guided): {output.outputs[0].text!r}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "2NSmACM_dRsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Control generated text using logits processor"
      ],
      "metadata": {
        "id": "FYZYM4qWfhjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "from tensorrt_llm import LLM\n",
        "from tensorrt_llm.sampling_params import LogitsProcessor, SamplingParams\n",
        "\n",
        "def text_to_token(tokenizer: PreTrainedTokenizer, text: str, last: boo):\n",
        "    tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    max_token_count = 1\n",
        "    bos_token_added = getattr(tokenizer, 'bos_token', None) and getattr(tokenizer, 'bos_token_id', None) in tokens\n",
        "    prefix_token_added = getattr(tokenizer, 'add_prefix_space',None) is not False\n",
        "\n",
        "    if bos_token_added or prefix_token_added:\n",
        "        max_token_count = 2\n",
        "\n",
        "    if not last and len(tokens) > max_token_count:\n",
        "        raise Exception(f\"Can't convert {text} to token. It has {len(tokens)} tokens.\")\n",
        "\n",
        "    return tokens[-1]"
      ],
      "metadata": {
        "id": "lNvQ14YufiJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GenLengthLogitsProcessor(LogitsProcessor):\n",
        "    \"\"\"\n",
        "    A logits processor that adjusts the likelihood of the end-of-sequence (EOS) token\n",
        "    based on the length of the generated sequence, encouraging or discouraging shorter answers.\n",
        "    WARNING: Create a new object before every model.generate call since token_count is accumulated.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer: The tokenizer used by the LLM.\n",
        "    boost_factor (float): A factor to boost the likelihood of the EOS token as the sequence length increases.\n",
        "                        Suggested value range is [-1.0, 1.0]. Negative values are used for the opposite effect.\n",
        "    p (int, optional): The power to which the token count is raised when computing the boost value. Default is 2.\n",
        "    complete_sentences (bool, optional): If True, boosts EOS token likelihood only when the last token is a full stop\n",
        "                                        or a new line. Default is False.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, boost_factor: float, p: int = 2, complete_sentences: bool = False):\n",
        "        self.eos_token = tokenizer.eos_token_id\n",
        "        self.boost_factor = boost_factor\n",
        "        self.p = p\n",
        "        self.token_count = 0\n",
        "        self.full_stop_token = text_to_token(tokenizer, \"It is a sentence.\", last=True)\n",
        "        self.new_line_token = text_to_token(tokenizer, \"It is a new line\\n\", last=True)\n",
        "        self.complete_sentences = complete_sentences\n",
        "\n",
        "    def __call__(self, req_ids: int, logits: torch.Tensor, ids: List[List[int]],\n",
        "                 stream_ptr, client_id: Optional[int]):\n",
        "        boost_val = self.boost_factor * (self.token_count**self.p) / (10** self.p)\n",
        "\n",
        "        stream = None if stream_ptr is None else torch.cuda.ExternalStream(\n",
        "            stream_ptr)\n",
        "\n",
        "        with torch.cuda.stream(stream):\n",
        "            ids = torch.LongTensor(ids).to(logits.device, non_blocking=True)\n",
        "\n",
        "            if self.complete_sentences:\n",
        "                enabled = (ids[:, -1] == self.full_stop_token) | (\n",
        "                    ids[:, -1] == self.new_line_token)\n",
        "                logits[:, :, self.eos_token] += enabled * boost_val\n",
        "            else:\n",
        "                logits[:, :, self.eos_token] += boost_val\n",
        "\n",
        "        self.token_count += 1"
      ],
      "metadata": {
        "id": "kYHQ8Zppi1Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    llm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "    # Sample prompts\n",
        "    prompts = [\n",
        "        \"The future of AI is\",\n",
        "        \"The future of AI is\",\n",
        "    ]\n",
        "\n",
        "    # Generate text\n",
        "    for prompt_id, prompt in enumerate(prompts):\n",
        "        if prompt_id % 2 == 0:\n",
        "            # Without logit processor\n",
        "            sampling_params = SamplingParams(top_p=1, max_tokens=200)\n",
        "        else:\n",
        "            # Each prompt can be specified with a logits processor at runtime\n",
        "            sampling_params = SamplingParams(\n",
        "                temperature=0.8,\n",
        "                top_p=0.95,\n",
        "                logits_processor=GenLengthLogitsProcessor(\n",
        "                    llm.tokenizer, boost_factor=1, complete_sentences=True))\n",
        "\n",
        "        output = llm.generate(prompt, sampling_params)\n",
        "        print(\n",
        "            f\"Prompt: {output.prompt!r}, Generated text: {output.outputs[0].text!r}\"\n",
        "        )\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "0aNcx2LRkeqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text with multiple LoRA adapters"
      ],
      "metadata": {
        "id": "1C7m7b4Zk7h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "from tensorrt_llm import LLM\n",
        "from tensorrt_llm.executor import LoRARequest\n",
        "from tensorrt_llm.llmapi import BuildConfig\n",
        "from tensorrt_llm.lora_manager import LoraConfig\n",
        "\n",
        "def main():\n",
        "  \"\"\"Generate text with multiple LoRA adapters\"\"\"\n",
        "      lora_dir1 = snapshot_download(repo_id=\"snshrivas10/sft-tiny-chatbot\")\n",
        "      lora_dir2 = snapshot_download(repo_id=\"givyboy/TinyLlama-1.1B-Chat-v1.0-mental-health-conversational\")\n",
        "      lora_dir3 = snapshot_download(repo_id=\"barissglc/tinyllama-tarot-v1\")\n",
        "\n",
        "      build_config = BuildConfig()\n",
        "      build_config.lora_config = LoraConfig(lora_dir=[lora_dir1])\n",
        "      llm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                enable_lora=True,\n",
        "                max_lora_rank=64,\n",
        "                build_config=build_config)\n",
        "      prompts = [\n",
        "        \"Hello, tell me a story: \",\n",
        "        \"Hello, tell me a story: \",\n",
        "        \"I've noticed you seem a bit down lately. Is there anything you'd like to talk about?\",\n",
        "        \"I've noticed you seem a bit down lately. Is there anything you'd like to talk about?\",\n",
        "        \"In this reading, the Justice card represents a situation where\",\n",
        "        \"In this reading, the Justice card represents a situation where\",\n",
        "      ]\n",
        "\n",
        "      for output in llm.generate(prompts,\n",
        "                               lora_request=[\n",
        "                                   None, LoRARequest(\"chatbot\", 1, lora_dir1),\n",
        "                                   None, LoRARequest(\"mental-health\", 2, lora_dir2),\n",
        "                                   None, LoRARequest(\"tarot\", 3, lora_dir3)\n",
        "                               ]):\n",
        "        prompt = output.prompt\n",
        "        generated_text = output.outputs[0].text\n",
        "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "fOuacCSck8eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speculative Decoding"
      ],
      "metadata": {
        "id": "VEwp3BwDnE2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "import click\n",
        "\n",
        "from tensorrt_llm import LLM, SamplingParams\n",
        "from tensorrt_llm.llmapi import (EagleDecodingConfig, MTPDecodingConfig, NGramDecodingConfig)\n",
        "\n",
        "prompts = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"What is the future of AI?\",\n",
        "]\n",
        "\n",
        "def run_MTP(model: Optional[str] = None):\n",
        "    spec_config = MTPDecodingConfig(num_nextn_predict_layers=1,\n",
        "                                    use_relaxed_acceptance_for_thinking=True,\n",
        "                                    relaxed_topk=10,\n",
        "                                    relaxed_delta=0.01)\n",
        "    llm = LLM(\n",
        "        # You can change this to a local model path if you have the model downloaded\n",
        "        model=model or \"nvidia/DeepSeek-R1-FP4\",\n",
        "        speculative_config=spec_config,\n",
        "    )\n",
        "    for prompt in prompts:\n",
        "        response = llm.generate(prompt, SamplingParams(max_tokens=10))\n",
        "        print(response.outputs[0].text)\n",
        "\n",
        "def run_Eagle3():\n",
        "    spec_config = EagleDecodingConfig(\n",
        "        max_draft_len=3,\n",
        "        speculative_model_dir=\"yuhuili/EAGLE3-LLaMA3.1-Instruct-8B\",\n",
        "        eagle3_one_model=True)\n",
        "\n",
        "    llm = LLM(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        speculative_config=spec_config,\n",
        "    )\n",
        "\n",
        "    for prompt in prompts:\n",
        "        response = llm.generate(prompt, SamplingParams(max_tokens=10))\n",
        "        print(response.outputs[0].text)\n",
        "\n",
        "def run_ngram():\n",
        "    spec_config = NGramDecodingConfig(\n",
        "        max_draft_len=3,\n",
        "        max_matching_ngram_size=3,\n",
        "        is_keep_all=True,\n",
        "        is_use_oldest=True,\n",
        "        is_public_pool=True,\n",
        "    )\n",
        "\n",
        "    llm = LLM(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        speculative_config=spec_config,\n",
        "        # ngram doesn't work with overlap_scheduler\n",
        "        disable_overlap_scheduler=True,\n",
        "    )\n",
        "\n",
        "    for prompt in prompts:\n",
        "        response = llm.generate(prompt, SamplingParams(max_tokens=10))\n",
        "        print(response.outputs[0].text)\n",
        "\n",
        "\n",
        "@click.command()\n",
        "@click.argument(\"algo\", type=click.Choice([\"MTP\", \"EAGLE3\", \"DRAFT_TARGET\", \"NGRAM\"]))\n",
        "@click.option(\"--model\", type=str, default=None, help=\"Path to the model or model name.\")\n",
        "\n",
        "def main(algo: str, model: Optional[str] = None):\n",
        "    algo = algo.upper()\n",
        "    if algo == \"MTP\":\n",
        "        run_MTP(model)\n",
        "    elif algo == \"EAGLE3\":\n",
        "        run_Eagle3()\n",
        "    elif algo == \"NGRAM\":\n",
        "        run_ngram()\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid algorithm: {algo}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "9DbXCJwknJIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run LLM-API with pytorch backend on Slurm"
      ],
      "metadata": {
        "id": "S2_5CxrspCmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export script=$SOURCE_ROOT/examples/llm-api/quickstart_advanced.py"
      ],
      "metadata": {
        "id": "kyEz-ukEpEaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "srun -l \\\n",
        "    --container-image=${CONTAINER_IMAGE} \\\n",
        "    --container-mounts=${MOUNT_DIR}:${MOUNT_DEST} \\\n",
        "    --container-workdir=${WORKDIR} \\\n",
        "    --export=ALL \\\n",
        "    --mpi=pmix \\\n",
        "    bash -c \"\n",
        "        $PROLOGUE\n",
        "        export PATH=$PATH:~/.local/bin\n",
        "        trtllm-llmapi-launch python3 $script \\\n",
        "            --model_dir $LOCAL_MODEL \\\n",
        "            --prompt 'Hello, how are you?' \\\n",
        "            --tp_size 2"
      ],
      "metadata": {
        "id": "Bk-Tb9LzpJo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run trtllm-bench with pytorch backend on Slurm"
      ],
      "metadata": {
        "id": "_TKibjxhpiUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export prepare_dataset=\"$SOURCE_ROOT/benchmarks/cpp/prepare_dataset.py\"\n",
        "export data_path=\"$WORKDIR/token-norm-dist.txt\"\n",
        "\n",
        "echo \"Preparing dataset...\"\n",
        "srun -l \\\n",
        "    -N 1 \\\n",
        "    -n 1 \\\n",
        "    --container-image=${CONTAINER_IMAGE} \\\n",
        "    --container-name=\"prepare-name\" \\\n",
        "    --container-mounts=${MOUNT_DIR}:${MOUNT_DEST} \\\n",
        "    --container-workdir=${WORKDIR} \\\n",
        "    --export=ALL \\\n",
        "    --mpi=pmix \\\n",
        "    bash -c \"\n",
        "        $PROLOGUE\n",
        "        python3 $prepare_dataset \\\n",
        "            --tokenizer=$LOCAL_MODEL \\\n",
        "            --stdout token-norm-dist \\\n",
        "            --num-requests=100 \\\n",
        "            --input-mean=128 \\\n",
        "            --output-mean=128 \\\n",
        "            --input-stdev=0 \\\n",
        "            --output-stdev=0 > $data_path"
      ],
      "metadata": {
        "id": "gQZVTBuhpi7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "echo \"Running benchmark...\"\n",
        "# Just launch trtllm-bench job with trtllm-llmapi-launch command.\n",
        "\n",
        "srun -l \\\n",
        "    --container-image=${CONTAINER_IMAGE} \\\n",
        "    --container-mounts=${MOUNT_DIR}:${MOUNT_DEST} \\\n",
        "    --container-workdir=${WORKDIR} \\\n",
        "    --export=ALL,PYTHONPATH=${SOURCE_ROOT} \\\n",
        "    --mpi=pmix \\\n",
        "    bash -c \"\n",
        "        set -ex\n",
        "        $PROLOGUE\n",
        "        export PATH=$PATH:~/.local/bin\n",
        "\n",
        "        # This is optional\n",
        "        cat > /tmp/pytorch_extra_args.txt << EOF\n",
        "print_iter_log: true\n",
        "enable_attention_dp: false\n",
        "EOF\n",
        "\n",
        "        # launch the benchmark\n",
        "        trtllm-llmapi-launch \\\n",
        "         trtllm-bench \\\n",
        "            --model $MODEL_NAME \\\n",
        "            --model_path $LOCAL_MODEL \\\n",
        "            throughput \\\n",
        "            --dataset $data_path \\\n",
        "            --backend pytorch \\\n",
        "            --tp 16 \\\n",
        "            --extra_llm_api_options /tmp/pytorch_extra_args.txt \\\n",
        "            $EXTRA_ARGS\n",
        "    \""
      ],
      "metadata": {
        "id": "jPfL06RGpnVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run trtllm-serve with pytorch backend on Slurm"
      ],
      "metadata": {
        "id": "WUwcMYo6pv0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "echo \"Starting trtllm-serve...\"\n",
        "# Just launch trtllm-serve job with trtllm-llmapi-launch command.\n",
        "srun -l \\\n",
        "    --container-image=${CONTAINER_IMAGE} \\\n",
        "    --container-mounts=${MOUNT_DIR}:${MOUNT_DEST} \\\n",
        "    --container-workdir=${WORKDIR} \\\n",
        "    --export=ALL,PYTHONPATH=${SOURCE_ROOT} \\\n",
        "    --mpi=pmix \\\n",
        "    bash -c \"\n",
        "        set -ex\n",
        "        $PROLOGUE\n",
        "        export PATH=$PATH:~/.local/bin\n",
        "\n",
        "        trtllm-llmapi-launch \\\n",
        "         trtllm-serve $LOCAL_MODEL \\\n",
        "            --tp_size 16 \\\n",
        "            --backend pytorch \\\n",
        "            --host 0.0.0.0 \\\n",
        "            ${ADDITIONAL_OPTIONS}"
      ],
      "metadata": {
        "id": "NTEYHh-WpwUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}